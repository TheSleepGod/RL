==================== Loading Models ====================
ModelManager initialized with base model path: /opt/tiger/model/Qwen2-7B
--- Loading and merging LoRA adapter using ModelManager ---
--- Method 1: Merging LoRA to model in memory ---
Loading base model from: /opt/tiger/model/Qwen2-7B
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.19it/s]
Loading LoRA adapter from: out/sft_qwen2_7b/final
Merging LoRA weights into the base model...
✅ Merge complete.
Wrapping loaded models with Value Head for PPO...
✅ Models wrapped successfully.
==========================================================

==================== Preparing Data ====================
[PPODataProcessor] Report for: /mnt/bn/search-nlp-us/haohanwen/haohw/data/all_data/gsm8k_train.jsonl
  - Total records read: 7473
  - Records processed (valid prompts): 7473
  - Prompts truncated due to overlength: 0
==========================================================
Initializing PPOTrainer...
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/bn/search-nlp-us/haohanwen/haohw/rl/train_ppo.py", line 165, in <module>
[rank0]:     main()
[rank0]:   File "/mnt/bn/search-nlp-us/haohanwen/haohw/rl/train_ppo.py", line 93, in main
[rank0]:     ppo_trainer = PPOTrainer(
[rank0]:                   ^^^^^^^^^^^
[rank0]: TypeError: PPOTrainer.__init__() got an unexpected keyword argument 'gamma'
