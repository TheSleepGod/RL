seed: 42
packing: false

model:
  name: Qwen/Qwen2-7B
  torch_dtype: bf16
  attn_impl: sdpa

data:
  train_path: data/train.jsonl
  val_path: data/val.jsonl
  max_seq_length: 2048
  response_template: "### 助手\n"

train:
  output_dir: out/sft_qwen2_7b
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 16
  num_train_epochs: 2
  learning_rate: 2.0e-4
  lr_scheduler_type: cosine
  warmup_steps: 500
  weight_decay: 0.01
  logging_steps: 20
  evaluation_strategy: steps
  eval_steps: 500
  save_steps: 500
  save_total_limit: 3
  bf16: true
  fp16: false
  gradient_checkpointing: true
  ddp_find_unused_parameters: false
  report_to: none

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  bias: "none"
  target_modules: ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]

quantization:
  load_in_4bit: false
